# youtube-sentiment-solution

YouTube Sentiment Analysis Solution
Heiner Buchholz
2023F CS 583-A hbuchhol@stevens.edu
10 December, 2023
1 Abstract
We are exploiting the ever-growing YouTube com- ments to provide an improved sentiment analysis solution aiming to improve the decision-making of content creators on media monitoring and brand marketing. Existing solutions use common natural language processing (NLP) techniques such as word- stemming, punctuation, and numerical characters removal which we attempt to challenge in this paper. We consider a method called ”lemmatizing” more nuanced since trimming the word endings often affects the word meaning. TextBlob assigns fixed sentiment scores from -1 to 1 to the words but we note that sentiment is context-dependent. Thus, it is crucial to fit curves that interpolate the scores using machine learning. We collected a dataset of 1.2 mil- lion comments with relatively few missing values [5]. We calculated word and punctuation count to gain initial insights. Bag of words (BOW) is considered a practical sentiment analysis approach although we observed that our models performed better with term frequency-inverse document frequency (TF-IDF). We were able to increase the number of features to maximize the model performance. The overall performance of the logistics regression model was good with some class imbalance which we handled with minority class oversampling. With the aid of YouTube Data API v3, we were able to fetch YouTube video data for predictive testing.
1.1 Github link
https://github.com/users/heinerbuchholz/pro jects/ 1/views/1
2 Introduction
We are building from existing sentiment analysis packages using data of YouTube comments to rate videos based on overall sentiment score. Text data is prepared for sentiment analysis with main natural language processing (NLP) libraries such as Natrual Language Toolkit (NLTK) and TextBlob. We were able to gather a large portion of YouTube public data containing video IDs, head comments in multiple languages, and their likes. Videos with no comments are handled with Fillna(””). Text data was visualized with seaborn maps and wordclouds. Our machine learning models run on numerous SKLearn packages.
It is a common practice for sentiment analysis papers to remove punctuation marks and numbers. While numbers and punctuation marks may still appear in terms with positive polarities such as ”5-star” or ”10 out of 10,” the sparsity of the terms suggests that the symbols are noise. We tried the models with and without the punctuation marks and numerical characters and the effects were minimal nonetheless. Other common practices in sentiment analysis include stemming which removes word endings such as suffixes. They help optimize the feature space size but sacrifice word meaning.
1
Hence, we decided to use lemmatization, similar to stemming but just reduces the words to their basic form preserving their meaning.
We employed common data cleansing practices such as replacing multiple spaces by single space and removing especial characters including emojies (we noted that TextBlob assigns emojies 0-polarity because they are considered especial Unicode char- acters :(). Data was prepared for TextBlob and it took over 3 minutes to assign the polarity scores. We observed that the modal comment was a single word without any punctuation mark which explains the little effect of removing punctuation marks. Punctuation marks are only slightly correlated with word count. Neutral words appear in both -1-polarity and 1-polarity comments along with the words that account for the polarity.
We built from existing sentiment analysis methods that use bag of words (BOW) to incorporate TF- IDF. TF-IDF is more computationally expensive but preserves word frequency which significantly affects the sentiment scores. By tuning the dataset slicing parameter, we were able to train a larger feature matrix. Logistic regression hyper-parameters were tuned with different approaches outperforming sup- port vector classifiers, a simpler sentiment analysis model. The large data size allowed us to train for more features. The class imbalance was fixed by including the 0-polarity comments in the negative class which optimized the area under the ROC curve (AUC ROC Score). Some other performance measures were great overall including the average precision score, accuracy score, and Brier score loss. The summary of the feature effects was consistent with other datasets. The overall prediction scores were more consistent with users’ ratings than TextBlob alone or Sachin Bisht’s sentiment analysis custom package [1]. We employed and edited the script to save YouTube comments in a file. Our solution also provides a summary of main effects at the comment-level.
2
3 Body 3.1 Method
We counted 231 entries with missing values after importing the data. Since the entries correspond to videos with no comments, the information is preserved by filling with ””. We counted over 125,000 comments with only one word (that’s about 10 % of the total comments) which suggests that our machine learning models will rely on the TextBlob polarity scores (Figure 1 below shows the word cloud of the sample data). The @mentions and special characters (including 709,200 emojies) were removed because they don’t count towards the scores. We counted almost 600,000 comments without any punctuation marks noting that the punctuation marks increase very slightly with the word count (see Figure 3). Nonetheless, we decided to preserve them along with the numerical characters due to the instances where they may affect the sentiment. After lemmatizing, the data was clean enough to make the TextBlob process more efficient reducing the running time to 3 minutes and 9.45 seconds. Note that the process was still intense as it involves tokenizing and assigning pre-trained sentiment scores to over a million comments (Figures 4 and 5 below show word clouds for the negative and positive words respectively).
 Figure 1: Word Frequencies

  3.2 Data
Figure 3: Heatmap
3.3 Tools & Technologies
Apple M2 came out in 2022 with 3.49 GHz and 10 core GPU. With just 16 GB of ram, it is able to fetch 100 K comments from YouTube in a few seconds. Sachin Bisht’s sentiment analysis custom modules are available on https://github.com/sachin- bisht/YouTube-Sentiment-Analysis/tree/master containing a YouTube comment extractor that uses the YouTube Data API v3 with a key we created on Google Cloud Console [1]. The script executes basic NLP tasks using NLTK which include tokenizing, handling stop words, and finding best bigrams with chi-square scoring. After removing punctuation marks, it generates a wordcloud of the video comments. The model is trained with 5331 negative and positive sentences each using multiple Naives Bayes classifiers, a popular choice for text classification. When tested, the classifier achieved an accuracy of approximately 78.17% which we would compare with our model.
Our script employs common Python 3 libraries for elementary tasks such as reading files, mea- suring model complexity, managing dataframes, manipulating arrays, and visualizing data. TextBlob
Figure 2: Histrograms
A portion of YouTube data was publicized on 2020 by Ramin Rahimzada on Kagle.com at https://www.kaggle.com/datasets/raminrahimzada/ youtube/data?select=channels.csv providing 322 MB of 43471 channels, 325292 videos and 1264035 comments [5]. It uses YouTube video IDs as key field which are part of the videos’ URL. The data contains the head comments only (the number of likes for each comment is of interest for assessing correlations with the comment polarity in future research). Other fields include authors’ information with 27 missing values, channels with 24 missing values, and published times which are not part of our current study. The likes were saved as int64 and the overall dataset uses just over 77.2+ MB of memory. The size of the dataset permitted us to employ up to 90 % of the data for training to train for more features, a factor that will positively impact the performance of our NLP models. 10 % of the data still represents 126,403 comments which we will employ for validation. Due to limited time and hardware resources, our approach necessitated the testing of our models on a per-video basis. The data, including the video IDs and comments, is fetched from YouTube directly with YouTube Data API v3 and Sachin Bisht’s sentiment analysis custom package that we have downloaded from our M2 console.
3

 Figure 4: Polarity -1.0 Worldcloud
the classic Shapley values from game theory. SHAP supports natural language models by incorporating the Shapley values to form games that explain large modern NLP models using few function evaluations [4].
3.4 Experiments
The word frequencies had a significant impact on the sentiment scores. BOW yielded an AUC ROC Score of 0.8482 in 894.35 seconds while TF-IDF yielded an AUC ROC Score of 0.9240 in 67.74 seconds. Since the polarity scores are continuous, they need to be categorized for our logistics regression classifier. An even class split was used assigning 0 (attributed to negative sentiment) for non-positive scores and 1 for positive scores (attributed to positive sentiments). After trying different heuristic techniques, the hyper parameters were tuned adding a regularization factor of C = 1000 and an stochastic gradient ascent method (SAGA) with up to 10,000 iterations. The model was optimized taking 1 minute and 7 seconds to train. It took 6 minutes and 23 seconds to plot the learning curve. The curve allowed us to train up to 3,000 features progressively increasing the model performance, point at which the lowest miss-classification error rate for the validation set almost met the highest miss-classification error rate for the training set at 100% of the training set usage which justifies the selection of the 90% data split (see figure 7).
3.5 Results
BOW approach was inefficient and did not beat the optimized model results in any aspect. After some heuristics setting the maximum degrees of freedom to 0.1 and minimum degrees of freedom to 1, the degrees of freedom of the data matrix did not significantly improve the results. Parameters were tuned as C = 1000, solver = ’saga’, and max iter = 10000 significantly improving the model performance. Table 1 contains the main results of
 Figure 5: Polarity +1.0 Wordcloud
0.17.1 processes our text data providing a simple API for NLP tasks such as sentiment analysis and classification. It possesses a feature that tokenizes our comments. NLTK 3.8.1 is for handling our main NLP tasks e.g. removing stop words and lemmatizing with appropriate packages. SKLearn is the main source of our model training. It contains packages for logistic regression, support vector machines, dataset splitting, BOW, TF-IDF, confusion matrices, classification reports, ROC, and precision-recall. Shapley additive explanations (SHAP) 0.44.0 represents a game theory approach to explain the output of machine learning models using
4

  Figure 6: Confusion Matrix the optimized model.
Figure 7: Learning Curve
video summarizing main effects at the comment-level.
3.6 Problems/Issues
We were unable to train support vector classifiers, a practical model for sentiment analysis. The parameter tuning and running time fell outside a reasonable frame. Although logistics regression is another common model for sentiment analysis, more complex models that handle continuous in- puts such as convolutional neural networks (CNN) could further capture intrinsic nuances in the word polarities [6]. We assigned the 0 polarities to the negative class to over-sample it which could lead to miss-interpretations in the results. 0-sentiment comments are attributed to ”neutrality” rather than negativity. Since a great deal of the com- ments have 0-polarity, it would be ideal to consider multi-nominal classification for a third class namely ’Neutral.’
More data may be needed to train more features. Although our model already performs well on our current data, there is no guarantee that it will make
 precision 0 0.96
1 0.95
accuracy
macro avg 0.95
recall f1-score 0.99 0.97 0.86 0.91
0.96 0.92 0.94 0.96 0.95
support 95064 31340 126404 126404 126404
 weighted avg 0.96
Table 1: Classification Report
 The overall scores and class balance look great. The average precision score was about 0.8561 and the Brier score loss 0.0447. Figures 8 and 9 show a relatively large area under the curve indicating good model performance. Figure 10 is a SHAP bee swarm of the terms that account for the highest variation in the predictions. Our final model can be used to pre- dict sentiment scores for either a single text or a set of comments from a YouTube video. Our script also provides a bee swarm plot of the video comments and a SHAP force plot for a particular comment in the
5

  Figure 8: Sentiment Class Prediction
accurate predictions on unseen words. Most of the YouTube data is in English creating high class imbalance in other languages. Removing stop words in only English can affect the prediction accuracy on data in other languages. Language detection packages would be necessary to split the text data by language. More time and resources are needed to gather the entire YouTube data which could provide analytics on video ranking by sentiment score. Another direction is the analysis of asso- ciations between our sentiment scores and like counts.
4 Conclusion
YouTube data can be fetched with free API and per- sonal GPU and easily pre-processed using Python 3 - NLTK. Large data is ideal for training multi- language NLP models. TextBlob sentiment scores can be used as input in multiple machine learning models such as support vector machines, logistic re- gression, Naive Bayes classifiers or CNN to predict application ratings. Hyper-parameters can be opti- mized using a TF-IDF matrix of the YouTube com- ments. We were able to improve a logistics regression model that performed great on sentiment analysis of
Figure 9: Sentiment Class Prediction
sample YouTube data. It is crucial though to fur- ther explore sentiment class imbalance and consider continuous inputs. Sentiment analysis is still an open field that has the potential to change how we leverage user-generated content for insightful and data-driven decision making.
5 References References
[1] Sachin-Bisht. (n.d.-a). Sachin-Bisht/youtube- sentiment-analysis: (unmaintained)fetch com- ments from the given video and determine sentiment towards the video is positive or negative. GitHub. https://github.com/sachin- bisht/YouTube-Sentiment-Analysis
[2] MayurDeshmukh10. (n.d.). Mayurdesh- mukh10/youtube analysis: Project for sen- timent analysis of YouTube comments and deep analysis of YouTube dataset. GitHub. https://github.com/MayurDeshmukh10/ youtube analysis/tree/master
6

 Figure 10: Effects Summary
[3] L-VinayKumar. (n.d.). L-VinayKumar/YouTube- sentiment-analysis: Sentiment analysis. GitHub. https://github.com/L-VinayKumar/YouTube- Sentiment-Analysis/tree/main
[4] Sentiment analysis with logistic regression. Sentiment Analysis with Logistic Regres- sion - SHAP latest documentation. (n.d.). https://shap.readthedocs.io/en/latest/example notebooks/tabularexamples/linear models/ Sentiment%20Analysis%20with%20Logistic% 20Regression.html
[5] Rahimzada, R. (2020, September 25). YouTube. Kaggle. https://www.kaggle.com/datasets/ raminrahimzada/youtube
[6] Mrdvince, (n.d.). (). sentiment-analysis. GitHub. https://github.com/mrdvince/sentiment- analysis
